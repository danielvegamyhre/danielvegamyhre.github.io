---
layout: post
title:  "Reducing Activation Recomputation in Large Transformer Models"
date:   2025-03-23 12:45:51 -0700
categories: ml performance
---

For session 9 of the Eleuther AI ML Scalability & Performance reading group, I presented the paper "Reducing Activation Recomputation in Large Transformer Models" from NVIDIA, which builds on the tensor parallel strategy introduced in Megatron-LM, with some additional techniques: sequence parallelism and selective activation recomputation.

My annotated versions of these papers can be found be found on my Github [here](https://github.com/danielvegamyhre/ml-scalability-and-performance-reading-group/tree/main/session_9).


Papers:
1. [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)


Note: you may have to disable ad blocker for the YouTube player to render correctly. Alternatively, you can watch the recording directly on YouTube [here](https://www.youtube.com/watch?v=9o2TXexHUh8).


<iframe width="560" height="315" src="https://www.youtube.com/embed/9o2TXexHUh8?si=QxGqv2DGL3r_W4_3" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>