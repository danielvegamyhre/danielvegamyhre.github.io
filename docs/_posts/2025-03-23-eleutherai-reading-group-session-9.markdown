---
layout: post
title:  "Reducing Activation Recomputation in Large Transformer Models"
date:   2025-03-23 12:45:51 -0700
categories: ml performance
---

For session 9 of the Eleuther AI ML Scalability & Performance reading group, I presented the paper "Reducing Activation Recomputation in Large Transformer Models" from NVIDIA, which builds on the tensor parallel strategy introduced in Megatron-LM, with some additional techniques: sequence parallelism and selective activation recomputation.

My annotated versions of these papers can be found be found on my Github [here](https://github.com/danielvegamyhre/ml-scalability-and-performance-reading-group/tree/main/session_9).


Papers:
1. [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)

Recording:

[![ML Scalability & Performance Reading Group Session 8: Megatron-LM](https://img.youtube.com/vi/9o2TXexHUh8/maxresdefault.jpg)](https://youtu.be/9o2TXexHUh8)
