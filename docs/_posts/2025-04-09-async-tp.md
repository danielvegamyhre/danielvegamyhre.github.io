---
layout: post
title:  "Async TP"
date:   2025-03-30 12:45:51 -0700
categories: ml performance
---
<script type="text/javascript"

  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>

In a [previous post](./2025-03-30-illustrated-megatron.md) we did a deep-dive into Megatron-style tensor parallelism. In this post, we'll look at an additional optimization building on this prior work: *asynchronous*  tensor parallelism, as described in the paper [Overlap Communication with Dependent Computation via
Decomposition in Large Deep Learning Models](https://dl.acm.org/doi/pdf/10.1145/3567955.3567959) and implemented in PyTorch.

## TL;DR

The goal of async TP is to enable full overlapping of the communication and computation that occur in tensor parallelism with sequence parallelism[^1], to achieve higher device utilization and speed up model training. These all-gather and reduce-scatter comms are typically "exposed" (not overlapped with computation), meaning their latency directly increases training step time and reduces device utilization.

Async TP achieves this overlapping in two ways:

1) Decomposing the blocking, multi-device NCCL collectives into a series of finer-grained P2P communications executed by the copy engine[^2], a dedicated hardware unit for direct memory access (DMA) which operates independently of SMs, preventing any SM contention issues or wave quantization magnification that may occur with SM-based comms kernels like NCCL send/recv[^3]

2) Decomposing the matmuls on each device in the TP group into a series of smaller matmuls, computed in the order the chunks of data arrive via the P2P comms.

In this post we will dive deeper into these concepts and tie them to the actual implementation in PyTorch to make things more concrete. The post is divided into the following sections:

- [Background: traditional TP + SP]
- [Fusing all-gather matmuls]
- [Fusing matmul reduce-scatters]
- [Bonus: improving perf further with quantization]

## Background: tensor parallelism with sequence parallelism
In a [previous post](./2025-03-30-illustrated-megatron.md) I did a deep dive into vanilla tensor paralellism (TP), but a subsequent [paper](https://arxiv.org/pdf/2205.05198) proposed another optimization on top of that: sequence parallelism (SP). 

When tensor parallelism is applied to MHA and FFN layers, the ops in between them (dropout, residual, layer norm) are identical, redundant computation on each device, which are cheap to compute but require a lot activation memory. The authors made the observation that these ops are **independent along the sequence dimension** (e.g., in layer norm we normalize along the feature dimension, it does not cross the batch or sequence dimensions). Therefore, we can actually parallelize the computation across the sequence dimension, while preserving mathematical fidelity with single device training. 

<insert diagram>

This reduces peak activation memory required on each device, and eliminates the redundant computation. 

However, there is still one downside to using TP + SP, which is the **exposed comms**.  When transitioning from a SP region to a TP region, we must **all-gather** the input activations (along the sequence dimension) then perform our usual row-wise/column-wise parallel GEMMs, then **reduce-scatter** the output activations 
(along the sequence dimension) to enter the subsequent SP region.

<img src="/images/async-tp/tp-sp.png" alt="tp-sp">


## Fusing all-gather matmuls

<img src="/images/async-tp/original-all-gather-matmul.png" alt="original-all-gather-matmul">

<img src="/images/async-tp/async-tp-all-gather-matmul-part1.png" alt="async-tp-all-gather-matmul-part1" style="width: 100%">

<img src="/images/async-tp/async-tp-all-gather-matmul-part2.png" alt="async-tp-all-gather-matmul-part2" style="width: 100%">

## Fusing matmul reduce-scatters

## Implementation details

[^1]: To learn more about the TP+SP parallelization strategy being optimized by async TP, you can read the original paper [Reducing Activation Recomputation in Large Language Models](https://arxiv.org/pdf/2205.05198) or watch my presentatin on the topic [here](https://www.youtube.com/watch?v=9o2TXexHUh8).
[^2]: See how copy engines fit into GPU architecture [here](https://old.hotchips.org/hc30/2conf/2.01_Nvidia_NVswitch_HotChips2018_DGX2NVS_Final.pdf)
[^3]: [Docs](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/p2p.html) on NCCL p2p send/recv ops
