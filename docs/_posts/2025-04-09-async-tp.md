---
layout: post
title:  "Deep dive: overlapping compute and comms in tensor parallelism"
date:   2025-03-30 12:45:51 -0700
categories: ml performance
---
<script type="text/javascript"

  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>

In a [previous post](./2025-03-30-illustrated-megatron.md) we did a deep-dive into Megatron-style tensor parallelism. In this post, we'll look at an additional optimization building on this prior work: *asynchronous*  tensor parallelism, as described in the paper [Overlap Communication with Dependent Computation via
Decomposition in Large Deep Learning Models](https://dl.acm.org/doi/pdf/10.1145/3567955.3567959) and implemented in PyTorch.

Note other methods[^4] have been proposed for achieving compute/comms overlap in TP, but this post will focus on the method natively supported in PyTorch, dubbed **async TP**.

## TL;DR

The goal of async TP is to enable full overlapping of the communication and computation that occur in tensor parallelism with sequence parallelism[^1], to achieve higher device utilization and speed up model training. These all-gather and reduce-scatter comms are typically "exposed" (not overlapped with computation), meaning their latency directly increases training step time and reduces device utilization.

Async TP achieves this overlapping in two ways:

1) Decomposing the blocking, multi-device NCCL collectives into a series of finer-grained P2P communications executed by the copy engine[^2], a dedicated hardware unit for direct memory access (DMA) which operates independently of SMs, preventing any SM contention issues or wave quantization magnification that may occur with SM-based comms kernels like NCCL send/recv[^3].

2) Decomposing the matmuls on each device in the TP group into a series of smaller matmuls, computed in the order the chunks of data arrive via the P2P comms.

In this post we will dive deeper into these concepts and tie them to the actual implementation in PyTorch to make things more concrete. The post is divided into the following sections:

- [Background: traditional TP + SP](#background-tensor-parallelism-with-sequence-parallelism)
- [Decomposing all-gather matmuls](#decomposing-all-gather-matmuls)
- [Decomposing matmul reduce-scatters](#decomposing-matmul-reduce-scatters)
- [Translating theory into practice: challenges at the hardware level](#translating-theory-into-practice-challenges-at-the-hardware-level)
- [Quantization](#quantization)
- [Implementation](#implementation-details)

## Background: tensor parallelism with sequence parallelism
In a [previous post](./2025-03-30-illustrated-megatron.md) I did a deep dive into vanilla tensor paralellism (TP), but a subsequent [paper](https://arxiv.org/pdf/2205.05198) proposed another optimization on top of that: sequence parallelism (SP). 

When tensor parallelism is applied to MHA and FFN layers, the ops in between them (dropout, residual, layer norm) are identical, redundant computation on each device, which are cheap to compute but require a lot activation memory. The authors made the observation that these ops are **independent along the sequence dimension** (e.g., in layer norm we normalize along the feature dimension, it does not cross the batch or sequence dimensions). Therefore, we can actually parallelize the computation across the sequence dimension, while preserving mathematical fidelity with single device training. 

This reduces peak activation memory required on each device, and eliminates the redundant computation. 


<img src="/images/async-tp/tp-sp.png" alt="tp-sp">


However, there is still one downside to using TP + SP, which is the **exposed comms**.  When transitioning from a SP region to a TP region, we must **all-gather** the input activations (along the sequence dimension) then perform our usual row-wise/column-wise parallel GEMMs, then **reduce-scatter** the output activations 
(along the sequence dimension) to enter the subsequent SP region.

We can see the exposed comms in by looking at a trace of a FFN forward pass in a [torchtitan](https://github.com/pytorch/torchtitan) training run with TP. Notice the all-gathers and reduce-scatters are **exposed.** 

<img src="/images/async-tp/vanilla-tp-trace.png" al="vanilla-tp-trace">

Exposed, blocking comms can result in lower device utilization and increased step time during training. How can we improve this?

## Decomposing all-gather matmuls

The authors of the paper [Overlap Communication with Dependent Computation via
Decomposition in Large Deep Learning Models](https://dl.acm.org/doi/pdf/10.1145/3567955.3567959) make the observation that a fully all-gathered matmul can be decomposed into 2 different submatmuls, the results of which are **concatenated.**  Let's walk through this step by step.

Let $$\textbf{A}$$ be the input activations, sharded row-wise:

$$
A = \begin{bmatrix} A_0 \\ A_1 \end{bmatrix}, 
\quad
A \in \mathbb{R}^{M \times K}, 
\quad
A_0, A_1 \in \mathbb{R}^{\frac{M}{2} \times K}
$$

Let $$\textbf{B}$$ be the weight matrix, sharded column-wise:

$$
B = \begin{bmatrix} B_0, B_1 \end{bmatrix},
\quad
B \in \mathbb{R}^{K \times N},
\quad
B_0, B_1 \in \mathbb{R}^{K \times \frac{N}{2}}
$$

In vanilla TP+SP, we all-gather $$\textbf{A}$$ on each device, and perform a matmul with the local shard of $$\textbf{B}$$ to produce a slice of the output $$\textbf{C}$$:

$$
C_0 = A \cdot B_0 = 
\begin{bmatrix}
C_{00} \\
C_{10}
\end{bmatrix}
,\quad
C_1 = A \cdot B_1 =
\begin{bmatrix}
C_{10} \\
C_{11}
\end{bmatrix}

\\

\text{where } C_0, C_1 \in \mathbb{R}^{K \times N}, \quad
C_{00}, C_{10}, C_{11} \in \mathbb{R}^{\frac{K}{2} \times N}

$$

Critically, the NCCL all-gather implementation is a blocking, multi-device operation utilizing SM-based comms. No device can begin their local matmul until the all-gather has completed on all devices!

The diagrams below visualize this process:

**Notation**: diagrams use the notation  $$A_{i} \cdot B_{j} = C_{ij}$$.

<img src="/images/async-tp/original-all-gather-matmul.png" alt="original-all-gather-matmul">

The key insight here is that we can actually compute $$A_0 \cdot B_0 = C_{00}$$ and $$A_1 \cdot B_0 = C_{01}$$ independently, there is no dependency between them. 

We can take advantage of this by computing each slice of $$\textbf{C}$$ one slice at a time, beginning each computation as soon as we finish pulling a given shard of $$\textbf{A}$$ over NVLink, rather than waiting to pull all shards of $$\textbf{A}$$.

Furthermore, if we have some mechanism to perform this data movement between devices **asynchronously**, then we can **overlap** the pulling of the next shard of $$\textbf{A}$$ with the computation using the current shard of A!

The high level algorithm from a single device's perspective works as follows:

1. Let device N begin with input activation shard $$A_N$$ present locally on the device.
2. Kick off async pull of the shard $$A_{N-1}$$  from device N-1.
3. Compute $$A_N \cdot B_N = C_{NN}$$ locally while async data movement is occurring. 
4. By the time this matmul is done, the async send/recv op has finished, and we can begin the next matmul immediately: $$A_{N-1} \cdot B_N = C_{N-1,N}$$.
5. Repeat steps 2-4 until all slices of $$\textbf{C}$$ have been computed.

The diagrams below visualize this process:

<img src="/images/async-tp/async-tp-all-gather-matmul-part1.png" alt="async-tp-all-gather-matmul-part1" style="width: 100%">

<img src="/images/async-tp/async-tp-all-gather-matmul-part2.png" alt="async-tp-all-gather-matmul-part2" style="width: 100%">

As you can see, the decomposed all-gather has mathematically equivalent results to the original, but with overlapped compute and comms!

Next, we'll look at how to decompose the matmul reduce-scatter patterns that occur in TP+SP.

## Decomposing matmul reduce-scatters
Decomposing matmul reduce-scatters is a bit tricker to understand, so let's take it step by step.


The key difference here compared to decomposed all-gather matmuls is that we pass around the **accumulator** (output buffer) rather than the shards of the input activations.

Note that the **input activations** to the matmul reduce-scatter are the **output activations** of our prior all-gather matmul. 

**Notation note**: diagrams use notation of $$C_{ij} \cdot D_{k} = E_{ijk}$$.

<img src="/images/async-tp/original-matmul-rs-1.png" alt="original-matmul-reduce-scatter-part1" style="width: 100%">

<img src="/images/async-tp/original-matmul-rs-2.png" alt="original-matmul-reduce-scatter-part1" style="width: 100%">

<img src="/images/async-tp/async-tp-matmul-rs-1.png" alt="async-tp-matmul-rs1" style="width: 100%">

<img src="/images/async-tp/async-tp-matmul-rs-2.png" alt="async-tp-matmul-rs2" style="width: 100%">

<img src="/images/async-tp/async-tp-matmul-rs-3.png" alt="async-tp-matmul-rs3" style="width: 100%">

As you can see, the final results of the decomposed matmul reduce-scatter are equivalent to that of the original, but with the communication overlapped with the computation!

## Translating theory into practice: challenges at the hardware level

Given this conceptual understanding, we now see that in theory we should be able to overlap the comms and compute for the all-gather matmul!

However, translating theory into practice can be non-trivial and full of unexpected oddities arising from implementation details of the hardware/software stack. So how can we implement such in an approach that actually improves upon vanilla TP+SP performance in practice?

## Implementation
TODO

## Quantization: improving 
TODO

[^1]: To learn more about the TP+SP parallelization strategy being optimized by async TP, you can read the original paper [Reducing Activation Recomputation in Large Language Models](https://arxiv.org/pdf/2205.05198) or watch my presentatin on the topic [here](https://www.youtube.com/watch?v=9o2TXexHUh8).
[^2]: See how copy engines fit into GPU architecture [here](https://old.hotchips.org/hc30/2conf/2.01_Nvidia_NVswitch_HotChips2018_DGX2NVS_Final.pdf)
[^3]: [Docs](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/p2p.html) on NCCL p2p send/recv ops
[^4]: [Distributed GEMM](https://blog.shi-labs.com/distributed-gemm-88be6a481e2b)