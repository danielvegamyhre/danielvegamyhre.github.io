---
layout: post
title: "Debugging deadlocks in warp-specialized GEMM kernels with CUDA-GDB"
date: 2026-02-02
---

While writing warp specialized GEMMs for Blackwell with CUDA + PTX, it has come to my attention that there are some great resources on efficient kernel designs for this architecture, but a lack of resources on debugging the often cryptic or even completely opaque CUDA errors that arise during the kernel development process. 

Classic "illegal memory access" errors are relatively straight-forward to resolve with [compute-sanitizer](https://docs.nvidia.com/compute-sanitizer/index.html). However, in kernels with a wide range of in-line PTX instructions, and asynchronous pipelines with complex synchronization patterns, these other CUDA errors became the bane of my existence:

1. `CUDA error: an illegal instruction was encountered` - which instruction? In some cases, I have many inline PTX instructions, [SMEM descriptors](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-shared-memory-descriptor), [instruction descriptors](https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-descriptor), etc., that could all result in illegal instruction errors. How can I quickly isolate the specific instruction that was illegal, and determine *why* it is illegal?
2. Last but not least: **deadlocks**. There are so many things that could cause this in warp specialized kernels on Blackwellk with producer warp, consumer warp, epilogue warp group, double buffered TMEM, persistent scheduling, and complex synchronization to orchestrate it all safely... how can we narrow down the issue *efficiently*?

This post includes some quick notes on using `cuda-gdb` to accelerate debugging each of these issues that I have picked up along the way, which hopefully some others will find useful as well.

Each section includes useful `cuda-gdb` commands, along with an example debugging of real issues I encountered while writing a warp-specialized persistent GEMM kernel with 2 CTA `tcgen05.mma` instructions for sm100. The kernel code for this is on Github [here](https://github.com/danielvegamyhre/gemm/blob/cbc6486103e1406423ca2eeffb89bc287066c2b3/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu). For background on these concepts, I recommend this excellent post [tcgen05 for dummies](https://gau-nernst.github.io/tcgen05) which was the inspiration for this kernel.

Table of Contents:
- [Illegal instruction](#illegal-instruction)
  - [Disassembling the instruction](#disassembling-the-instruction)
  - [Examining instruction operands](#examining-instruction-operands)
- [Deadlocks](#deadlocks)
  - [Finding the Hung Thread](#finding-the-hung-thread)
  - [Navigating the Call Stack](#navigating-the-call-stack)
  - [Investigating the Consumer Warp](#investigating-the-consumer-warp)
  - [Inspecting Local Variables](#inspecting-local-variables)
  - [Investigating the Producer Warp](#investigating-the-producer-warp)
  - [Narrowing Down the Issue](#narrowing-down-the-issue)
  - [The Fix](#the-fix)

**Prerequisite**: make sure to compile CUDA kernels with `-G` (debug info for device code) and `-g` (debug info for host code)!

## Illegal instruction

### TL;DR

1. Run your code with `cuda-gdb <executable>` or `cuda-gdb --args <program>` (e.g. `cuda-gdb --args pytest test.py`)
2. Once the interactive session opens, use `"r"` to run the program.
3. (observe crash)
4. `cuda-gdb` catches the halt and it shows `"triggered at <PC address>"` in the terminal session
5. Use command `disas <PC address>, +16`:
   - This disassembles the 16 bytes after the program counter address into human readable SASS instruction output, showing the instruction that caused the crash. 16 bytes is the width of an instruction on the sm100 architecture.


Below is a real example where I used this:

### Disassembling the instruction

1. `disas <PC address>, +16`  
```bash
... 
UTMALDG.3D [UR8][UR4]  
```

This is a 3d TMA async load from GMEM to SMEM. UR8 and UR4 are **uniform registers**, meaning unlike regular registers `R0`, `R1`, etc., they are shared across all threads in a warp.

Why is this instruction illegal? Let's inspect the operands:

### Examining instruction operands

2. `info register UR8 UR4 `
```bash
... 
UR8  0x460 
```
UR4 was a large value that looked like the 64-bit generic address of the Tensormap used for the TMA load.
UR8 was a small value that looked like the 32-bit SMEM **shared address** of the destination.

Since I am using `CU_TENSOR_MAP_SWIZZLE_128B` in my Tensormap for the TMA load, the destination SMEM address must be 1024-byte aligned.

Converting hex to decimal, we see 0x460 = 1120 is **not 1024-byte aligned!** This narrows down the issue to how we calculate the SMEM offset
for each A/B tile in SMEM, and we're able to more quickly resolve the issue from there.

## Deadlocks

### TL;DR

1. Run your code with `cuda-gdb <executable>` or `cuda-gdb --args <program>` (e.g. `cuda-gdb --args pytest test.py`)
2. Once the interactive session opens, use `"r"` to run the program.
3. Once you hit the hang, hit `Ctrl+C` to send a SIGINT signal, which will kill the python thread and yield back control to `cuda-gdb`.
4. `cuda-gdb` session should now be in the **thread context** of a hung thread.
5. Use `cuda thread (x, y, z)` to switch to another thread context as necessary (e.g. thread in producer vs consumer warps)
6. Navigate the call stack with `up` or `down` to figure out where that is in its execution lifetime, etc.
7. Use `print <variable>` to print variables.
8. Combining these tools, you should be able to narrow down the issue and accelerate the debugging process. 

Other useful `cuda-gdb` commands for this include:
- `set cuda printf_flushing on` - flush printf calls when a GPU breakpoint is hit, to be sure you can see them.
- `set cuda break_on_launch application` - break on the first instruction of the kernel, allowing you to set additional breakpoints in the command line and more
- `break <file>:<line> if <condition>` - set a conditional breakpoint for a particular thread (e.g., `threadIdx.x == 0 && block_id > start_block_id`)
- `cuda info clusters` - info on threadblock clusters (on Hopper and later)
- `cuda info barriers` - info on barrier objects
 

Below is a concrete example of how this helped me debug a deadlock:

### Finding the Hung Thread

Run CUDA GDB. When it hangs, hit `Ctrl+C`, which will halt on one of the hung threads. Note the block ID, warp ID, and thread ID in the output. For warp-specialized kernels, this is critical:

```bash
[New Thread 0x7ffe301c1640 (LWP 19512)]
[New Thread 0x7ffe30bc2640 (LWP 19513)]
...

^C
Thread 1 "python" received signal SIGINT, Interrupt.
[Switching focus to CUDA kernel 0, grid 4, cluster (0,0,0), cluster dim (2,1,1), block (0,0,0), thread (0,0,0), device 0, sm 142, warp 1, lane 0]
0x00007ffe439bac10 in mbarrier_try_wait_parity (mbar_addr=197728, parity=0)
    at /home/dvm/gemm/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu:134
134             : "r"(mbar_addr), "r"(parity) // inputs
```

Notice in the output we can see the line number in our source code where it's hung, which corresponds to an inline PTX instruction [mbarrier.try_wait.parity.acquire.cta.shared::cta.b64](https://github.com/danielvegamyhre/gemm/blob/cbc6486103e1406423ca2eeffb89bc287066c2b3/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu#L134). This is a non-blocking test of the parity bit of the mbarrier object we are using for synchronization, which we check in a loop until the bit has flipped, signaling completion.

### Navigating the Call Stack

Use `up` to get to a more informative part of the call stack:

```bash
(cuda-gdb) up

#1  mbarrier_wait_parity<<<(148,1,1),(192,1,1)>>> (mbar_addr=197728, parity=0)
    at /home/dvm/gemm/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu:141
141       while (!mbarrier_try_wait_parity(mbar_addr, parity)) {
```

```bash
(cuda-gdb) up

#2  ws_gemm_2cta_mma<6, 128, 256, 64, 128, 256, 16><<<(148,1,1),(192,1,1)>>> (a_map=<error reading variable: Cannot access memory at address 0x0>,
    b_map=<error reading variable: Cannot access memory at address 0x80>, C=0x7ffbf4000000, M=4096, N=4096, K=4096)
    at /home/dvm/gemm/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu:538
538                 mbarrier_wait_parity(mma_mbar_addr, mma_parity);
```

I can now see that for warp 1 (epilogue warp), it's hung waiting for the MMA mbarrier that signals `tcgen05.mma` completion.

### Investigating the Consumer Warp

Let's see what's going on with the MMA warp. You can switch thread context like so:

```
cuda thread (160,0,0)
```

This switches thread context to the first thread of the MMA warp (warp ID 5), which is the thread that issues `tcgen05.mma` and then the `tcgen05.commit` multicast with the `mma_mbar` to signal the producer warp when the commit group of MMAs has completed, and the SMEM buffers for A/B tiles can be safely re-used.

Again, we start here by using `up` to get to a more useful part of the call stack:

```bash
(cuda-gdb) cuda thread (160,0,0)
[Switching focus to CUDA kernel 0, grid 4, cluster (0,0,0), cluster dim (2,1,1), block (0,0,0), thread (160,0,0), device 0, sm 142, warp 6, lane 0]
0x00007ffe439b70e0      134             : "r"(mbar_addr), "r"(parity) // inputs
```

```bash
(cuda-gdb) up
#1  mbarrier_wait_parity<<<(148,1,1),(192,1,1)>>> (mbar_addr=197672, parity=1)
    at /home/dvm/gemm/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu:141
141       while (!mbarrier_try_wait_parity(mbar_addr, parity)) {
```

```bash
(cuda-gdb) up
#2  ws_gemm_2cta_mma<6, 128, 256, 64, 128, 256, 16><<<(148,1,1),(192,1,1)>>> (a_map=<error reading variable: Cannot access memory at address 0x0>,
    b_map=<error reading variable: Cannot access memory at address 0x80>, C=0x7ffbf4000000, M=4096, N=4096, K=4096)
    at /home/dvm/gemm/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu:483
483                     mbarrier_wait_parity(smem_full_mbar_addr + consumer_next_buf * sizeof(uint64_t), smem_full_parity[consumer_next_buf]);
```

### Inspecting Local Variables

Hmm, the MMA warp is hung waiting for the mbarrier that signals a particular shared memory buffer is ready for use. We can check which iteration this happens on by printing local variables like so:

```
(cuda-gdb) print block_k_idx
$1 = 59

(cuda-gdb) print num_blocks_k
$2 = 64
```

Interesting, it doesn't happen immediately in the main loop - rather, the hang happens on a seemingly random block K index, which defines which BK tile we're at along K in the main/inner loop when computing the output block. 

This gives us useful information, that we are successfully processing many BK tiles in the inner loop before hanging, meaning the whole synchronization pipeline is working as expected for part of the lifetime, but hitting a bug at some point.

### Investigating the Producer Warp

Why isn't the consumer receiving the signal from the producer that the next SMEM buffer A/B tiles are ready for use? 

Let's check the producer, warp ID 4:

```
(cuda-gdb) cuda thread (128,0,0)

[Switching focus to CUDA kernel 0, grid 4, cluster (0,0,0), cluster dim (2,1,1), block (0,0,0), thread (128,0,0), device 0, sm 142, warp 5, lane 0]
0x00007ffe439b4a30      137       return static_cast<bool>(wait_complete);
```

Navigate through the call stack:
```bash
(cuda-gdb) up

#1  mbarrier_wait_parity<<<(148,1,1),(192,1,1)>>> (mbar_addr=197720, parity=1)
    at /home/dvm/gemm/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu:141
141       while (!mbarrier_try_wait_parity(mbar_addr, parity)) {
```

```bash
(cuda-gdb) up

#2  ws_gemm_2cta_mma<6, 128, 256, 64, 128, 256, 16><<<(148,1,1),(192,1,1)>>> (a_map=<error reading variable: Cannot access memory at address 0x0>,
    b_map=<error reading variable: Cannot access memory at address 0x80>, C=0x7ffbf4000000, M=4096, N=4096, K=4096)
    at /home/dvm/gemm/blackwell/tcgen05_persistent_2cta_warp_specialized/tcgen05_persistent_2cta_warp_specialized.cu:430
430                         mbarrier_wait_parity(smem_empty_mbar_addr + producer_next_buf * sizeof(uint64_t), smem_empty_parity[producer_next_buf]);
```

We can see the producer warp is hung waiting for the signal that a shared memory buffer is ready to be re-used. Hmm, this seems to be the start of the hang - the whole pipeline grinds to a halt because the producer isn't producing anything.

### Narrowing Down the Issue

This narrows down the issue to 2 general areas to investigate:

1. Consumer warp signaling `smem_empty` mbarrier improperly
2. Producer warp checking `smem_empty` mbarrier improperly

To get over the finish line, we do need some old-fashioned code analysis and debugging, but we have **drastically** narrowed down the possibility space. 

In this case, the fact that this synchronization pipeline worked properly with a non-persistent kernel (i.e., no grid-strided loop over blocks) suggested I should look for what could change when iterating to a new block. After tracing through the code, I noticed the condition the producer uses to check the `smem_empty` mbarrier assumed only 1 output tile per block:

```c++
if (block_k_idx >= QUEUE_SIZE)
{
    mbarrier_wait_parity(smem_empty_mbar_addr + producer_next_buf * sizeof(uint64_t), smem_empty_parity[producer_next_buf]);
    smem_empty_parity[producer_next_buf] ^= 1;
}
```

Critically, `block_k_idx` resets to 0 for every new block, and we will skip waiting on the `smem_empty` mbarrier and flipping the parity, which will get our producer and consumer out of sync!

### The Fix

The fix is simple: wait on the mbarrier once we start re-using SMEM queue buffers within the first output tile, AND for every iteration of the 2nd+ output tiles.

```diff
- if (block_k_idx >= QUEUE_SIZE)
+ if (block_k_idx >= QUEUE_SIZE || bid > start_bid) { ... }
{
    mbarrier_wait_parity(smem_empty_mbar_addr + producer_next_buf * sizeof(uint64_t), smem_empty_parity[producer_next_buf]);
    smem_empty_parity[producer_next_buf] ^= 1;
}
```

## Conclusion

Check out the `cuda-gdb` [docs](https://docs.nvidia.com/cuda/cuda-gdb/index.html#) for more details than the basic workflows I outlined here. I put it off for far too long, and regret the time wasted with AI hallucinations and staring at code over and over!